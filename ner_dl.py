import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
# -*- coding: utf-8 -*-
"""ner_dl.ipynb

Automatically generated by Colaboratory.

Original file is located at
tf
https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_dl.ipynb

![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_dl.ipynb)

# NER with Deep Learning
"""

# Only run this cell when you are using Spark NLP on Google Colab
# !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash

"""In the following example, we walk-through a LSTM NER model training and prediction. This annotator is implemented on top of TensorFlow.

This annotator will take a series of word embedding vectors, training CoNLL dataset, plus a validation dataset. We include our own predefined Tensorflow Graphs, but it will train all layers during fit() stage.

DL NER will compute several layers of BI-LSTM in order to auto generate entity extraction, and it will leverage batch-based distributed calls to native TensorFlow libraries during prediction.

#### 1. Call necessary imports and set the resource folder path.
"""

import os
import sys

from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark import SparkContext as sc
from sparknlp.annotator import *
from sparknlp.common import *
from sparknlp.base import *

import time
import zipfile

# sc.setLogLevel("ERROR", "ERROR")
"""#### 2. Download CoNLL 2003 data if not present"""

# Download CoNLL 2003 Dataset
import os
from pathlib import Path
import urllib.request
url = "https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/"
file_train="eng.train"
file_testa= "eng.testa"
file_testb= "eng.testb"
# https://github.com/patverga/torch-ner-nlp-from-scratch/tree/master/data/conll2003
if not Path(file_train).is_file():
    print("Downloading "+file_train)
    urllib.request.urlretrieve(url+file_train, file_train)
    
if not Path(file_testa).is_file():
    print("Downloading "+file_testa)
    urllib.request.urlretrieve(url+file_testa, file_testa)

if not Path(file_testb).is_file():
    print("Downloading "+file_testb)
    urllib.request.urlretrieve(url+file_testb, file_testb)
"""#### 4. Create the spark session"""

import sparknlp

spark = sparknlp.start()
# sc.setLogLevel(spark, "ERROR")
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

 
# import tensorflow as tf
# tf.get_logger().setLevel('ERROR')

# print("Spark NLP version: ", sparknlp.version())
# print("Apache Spark version: ", spark.version)


"""#### 6. Load parquet dataset and cache into memory"""

from sparknlp.training import CoNLL
conll = CoNLL(
    # documentCol="document",
    # sentenceCol="sentence",
    # tokenCol="token",
    # posCol="pos"
)
documentAssembler = DocumentAssembler() \
    .setInputCol("text") \
    .setOutputCol("document")

sentence = SentenceDetector() \
    .setInputCols(["document"]) \
    .setOutputCol("sentence")

tokenizer = Tokenizer() \
    .setInputCols(["sentence"]) \
    .setOutputCol("token")

# Then the training can start

embeddings = BertEmbeddings.pretrained() \
    .setInputCols(["sentence", "token"]) \
    .setOutputCol("embeddings")

nerTagger = NerDLApproach() \
    .setInputCols(["sentence", "token", "embeddings"]) \
    .setLabelColumn("label") \
    .setOutputCol("ner") \
    .setMaxEpochs(1) \
    .setRandomSeed(0) \
    .setVerbose(0)

pipeline = Pipeline().setStages([
    embeddings,
    nerTagger
])

# We use the sentences, tokens, and labels from the CoNLL dataset.

# conll = CoNLL()
# trainingData = conll.readDataset(spark, "src/test/resources/conll2003/eng.train")
# training_data = conll.readDataset(spark, './eng.train').transform()
# testAData= conll.readDataset(spark, './eng.testa')
# pipelineModel = pipeline.fit(training_data )

testBData= conll.readDataset(spark, './eng.testb')


# embeddings = WordEmbeddingsModel.pretrained()\
# .setOutputCol('embeddings')

# ready_data = embeddings.transform(training_data)
# ready_testAData = embeddings.transform(testAData)
# ready_testBData = embeddings.transform(testBData)

# ready_data.show(4)
ner_model= NerDLModel.pretrained()

"""#### 9. Lets predict with the model"""

document = DocumentAssembler()\
    .setInputCol("text")\
    .setOutputCol("document")

sentence = SentenceDetector()\
    .setInputCols(['document'])\
    .setOutputCol('sentence')

token = Tokenizer()\
    .setInputCols(['sentence'])\
    .setOutputCol('token')

embeddings = WordEmbeddingsModel.pretrained()\
.setOutputCol('embeddings')

prediction_pipeline = Pipeline(
    stages = [
        document,
        sentence,
        token,
        embeddings,
        ner_model
    ]
)
import pandas as pd
df = pd.read_csv("./datasets/conll2003/conll2003.csv")
dfList = []
# for i in range(10):
    # print("cross val ", i)
    # test_df = df.sample(frac=0.1)
    # test_df = test_df.assign(textList = test_df.text)
    # test_df.textList  = test_df.textList .apply(lambda x : [x])
    # test_df = test_df.assign(labelsList = test_df.labels.str.split())
# print(test_df)
# test_df.to_csv("test.csv", sep=",", index=False)


    # prediction_data = spark.createDataFrame(test_df.textList .values.tolist()).toDF("text")
# prediction_data.show()
    # prediction_model = prediction_pipeline.fit(prediction_data)
# prediction_model.transform(prediction_data).show()
# if not os.path.isdir("./results/") : os.makedirs("./results/")

# We can be fast!

    # lp = LightPipeline(prediction_model)
# print(dir(prediction_model))
# print(dir(lp))
    # results = []
    # skipped = 0

# print(res)

    # for index, row in test_df.iterrows():
        # result = lp.annotate(row["text"])
        # if len(result['ner']) != len(row['labelsList']) :
            # skipped +=1
            # continue

        # truePerc = 0
        # for j in range(len(row["labelsList"])):
            # if row["labelsList"][j] == result["ner"][j] : truePerc +=1
        # truePerc = round(truePerc / len(row["labelsList"]) * 100, 2)
        # res = True
        # if truePerc < 100 : res = False
        # tmp = {"text":row["text"], "labelsList":row["labelsList"], "predictedLabels":result['ner'], "accuracy": truePerc, "res": res}
        # results.append(tmp)


    # eval_df = pd.DataFrame(results)
    # eval_df  = eval_df .assign(globalAccuracy = round(len(eval_df [eval_df ["res"] == True]) / len(eval_df ) * 100, 2))
    # eval_df.to_csv("./results/eval_spark_" + str(i) + ".csv", sep=",", index=False)
    # dfList.append(eval_df) 
# dfList = [pd.read_csv("results/" + file) for file in os.listdir("results/")]
# print("concatenation")
# finalDf = pd.concat(dfList, ignore_index=True).drop_duplicates(subset="text")
# finalDf = finalDf.assign(meanAccuracy = finalDf.accuracy.mean())
# finalDf = finalDf.assign(medianAccuracy = finalDf.accuracy.median())
# finalDf = finalDf.assign(meanGlobalAccuracy = finalDf.globalAccuracy.mean())
# finalDf = finalDf.assign(medianGlobalAccuracy = finalDf.globalAccuracy.median())
# print(len(finalDf))
# print("minimal accuracy :", finalDf.accuracy.min())
# print("mean accuracy :", finalDf.accuracy.mean())
# print("median accuracy :", finalDf.accuracy.median())
# print("minimum global accuracy : ", finalDf.globalAccuracy.min())
# print("maximum global accuracy : ", finalDf.globalAccuracy.max())
# print("mean global accuracy :", finalDf.globalAccuracy.mean())
# print("median global accuracy :", finalDf.globalAccuracy.median())
# finalDf.to_csv("./results/finalSpark_eval.csv", sep=",", index=False)

# print("starting single entity evaluation")
df1 = pd.read_csv("eng.train", sep=" ", names=["words", "wordType", "idk", "entityType"])
df2 = pd.read_csv("eng.testa", sep=" ", names=["words", "wordType", "idk", "entityType"])
df3 = pd.read_csv("eng.testb", sep=" ", names=["words", "wordType", "idk", "entityType"])
fullDf = pd.concat([df1, df2, df3], ignore_index=True)
print(len(fullDf .entityType.unique()))
print(fullDf .entityType.unique())
dfList = []
# for i in range(10):
    # print("starting single entity evaluation round : ", i)
    # balanced_subsample = fullDf.groupby('entityType', group_keys=False).apply(lambda x: x.sample(frac=0.2))
    # balanced_subsample = balanced_subsample.assign(text= balanced_subsample.words)
    # balanced_subsample.text= balanced_subsample.text.apply(lambda x : [x])
    # print(balanced_subsample .words.values.tolist())
    
    # prediction_data = spark.createDataFrame(balanced_subsample.text.values.tolist()).toDF("text")
# prediction_data.show()
    # prediction_model = prediction_pipeline.fit(prediction_data)
    # lp = LightPipeline(prediction_model)
    # results = []
    # for index, row in balanced_subsample.iterrows():
        # result = pd.NA
        # try:
            # result = lp.annotate(str(row["words"]))["ner"][0]
        # except:
            # print("word causing issue : ", row["words"])
            
        # results.append(result)
            
    # result = lp.annotate(balanced_subsample .words.values.tolist())["ner"]
    # eval_df = balanced_subsample.assign(predictedEntityType = results)
    # eval_df.predictedEntityType  = eval_df.predictedEntityType.apply(lambda x: x[0] if len(x) == 1 else x) 
    # eval_df["predictionResult"] = eval_df.entityType == eval_df.predictedEntityType
    # for label in eval_df.entityType.unique():
        # column_name = f'accuracy_{label}'
        # values_to_assign = round(len(eval_df[eval_df.entityType == label & eval_df.predictionResult == True]) / len(eval_df[eval_df.entityType == label]) *100, 2)
        # eval_df = eval_df.assign(**{column_name: values_to_assign})
    # print(eval_df)
    # eval_df.to_csv("results/eval_spark_unique_token_" + str(i) + ".csv", sep=",", index=False)
    
    


import seaborn as sns
import matplotlib.pyplot as plt

def plotAccuracy(df, model = "sparknlp", datasetName = "conll-2003", saveFolder = "results/figures/"):
    if not os.path.isdir("results/") : os.makedirs("results/")
    if not os.path.isdir("results/figures/") : os.makedirs("results/figures/")
    
    # Extract column names ending with '_accuracy'
    accuracy_columns = [col for col in df.columns if col.startswith('accuracy_')]

    # Set up Seaborn with a diverging color palette
    sns.set(style="whitegrid")
    colors = sns.color_palette("RdYlGn", len(accuracy_columns))

    # Plotting the bar chart with seaborn
    plt.figure(figsize=(10, 6))
    sns.barplot(x=accuracy_columns, y=df[accuracy_columns].values.flatten(), palette=colors)

    # Customize plot appearance
    plt.title(model + ' Accuracy Comparison for ' + datasetName, fontsize=16)
    plt.xlabel('Entity type', fontsize=14)
    plt.ylabel('accuracy', fontsize=14)
    plt.ylim(0, 100)  # Set y-axis limit to percentages (0-100)

    # Add legend
    plt.legend([col.replace("_", "").replace("accuracy", "") for col in accuracy_columns], title="Labels List", loc="upper left")

    # Add grid lines
    plt.grid(True, linestyle='--', alpha=0.7)

    # Increase tick label font size
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    plt.show()
    plt.savefig(f'{saveFolder}{model}_accuracy_plot_for_{datasetName}.png')

dfList = [pd.read_csv("results/" + file) for file in os.listdir("results/") if "unique" in file]
for df in dfList:
    print("global accuracy : ", round(len(df[df.predictionResult == True]) / len(df) * 100, 2))
    df = df.assign(accuracy_global = round(len(df[df.predictionResult == True]) / len(df) * 100, 2))
    print(len(df.entityType.unique()))
    print(df.entityType.unique())
    # df.predictionResult = df.predictionResult .replace(True, 1)
    # df.predictionResult = df.predictionResult .replace(False, 0)
    
    for label in df.entityType.unique():
        tmp = df[df.entityType == label]
        accuracy = round(len(tmp[tmp.predictionResult == True]) / len(tmp) * 100, 2)
        
        column_name = f'accuracy_{label}'
        # values_to_assign = round(len(df[df.entityType == label & df.predictionResult == 1]) / len(df[df.entityType == label]) *100, 2)
        print(f'accuracy for {column_name} : ', str(accuracy))
        df = df.assign(**{column_name:accuracy})
        
    plotAccuracy(df)
